<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>ML Project Poster</title>
        <link rel="stylesheet" href="style.css" />

        <meta name="description" content="CMS 2025 ML group poster" />

        <script defer init src="script.js"></script>

        <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <header id="header">
            <h1>Machine Learning</h1>
            <h2></h2>
            <p>TM</p>
        </header>
        <div id="cms-logo-wrapper">
            <a href="https://cms.tela.org.uk">
                <img
                    id="cms-logo-img"
                    width="165"
                    height="70"
                    src="https://cms.tela.org.uk/wp-content/uploads/2022/02/CMS-Web-Version7.svg"
                    alt="CMS Logo" />
            </a>
        </div>
        <div id="content">
            <div class="column">
                <div class="header"><h2 class="header">Brief overview</h2></div>
                <div class="text">
                    <p>
                        <b>Machine learning</b> is a subset of <b>Artificial Intelligence</b> which focuses on developing algorithms capable of
                        "learning" from data and improving their performance over time without being explicitly programmed to do so. These algorithms
                        can identify patterns and make predictions or decisions based on the input data, making machine learning a powerful tool in
                        various applications.
                    </p>
                </div>
                <div class="header"><h2 class="header">Neural Networks</h2></div>
                <div class="text">
                    <p>
                        Neural networks are a type of Machine Learning which is inspired by inspired by the structure and function of biological
                        neural networks, in brains<sup
                            ref
                            author="Wikipedia"
                            date="03/03/2025"
                            src="https://en.wikipedia.org/wiki/Neural_network_%28machine_learning%29"></sup>
                    </p>
                </div>
                <div class="img">
                    <img
                        width="200"
                        src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/375px-Colored_neural_network.svg.png"
                        alt="Alt Text (image example)" />
                    <p class="caption">
                        <sup ref author="Wikipedia" date="24/04/2025" src="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)"></sup
                        >Example of a small neural network
                    </p>
                </div>
                <div class="header"><h3 class="header">Backpropagation</h3></div>
                <div class="text">
                    <p>
                        <!-- Backpropagation is a method used for gradient estimation<sup
                            ref
                            author="Wikipedia"
                            date="24/04/2025"
                            src="https://en.wikipedia.org/wiki/Backpropagation"></sup
                        >, which is key for training a neural network. It estimates the gradient of the <b>loss function</b> for each layer, starting
                        from the output and ending at the input (each dimention of the function is a weight/bias in the neurons of a layer, meaning it
                        can have huge numbers of dimentions) to find how to tweak each parameter (weight or bias) to minimise the function (i.e get
                        the results we want.) -->
                        Backpropagation is an algorithm used to train neural networks by minimising the cost function, often Mean Squared Error (MSE). The cost function for one training example: \[ C = \frac{1}{2} \sum_{j=1}^{m} (a_j^L - y_j)^2 \]
                        It works by calculating the partial derivatives of the cost function with respect to each weight and bias using the chain rule [reference to 3b1b]. The weights and biases are then updated using gradient descent. This involves subtracting a fraction of the gradient, scaled by the learning rate (a hyperparameter controlling the size of the updates<sup
                        ref
                        author=""
                        date="31/03/2025"
                        src="https://www.geeksforgeeks.org/impact-of-learning-rate-on-a-model/"></sup>), over multiple iterations to minimise the cost function, gradually improving its accuracy over time.<sup
                        ref
                        author="Nielsen MA"
                        date="30/03/2025"
                        src="http://neuralnetworksanddeeplearning.com/chap2.html"></sup>
                    </p>

                    <p>
                        In order to compute the partial derivatives we need, an intermediate quantity \( \delta ^{l}_{j} \) is introduced, defined as the error in the j<sup>th</sup> neuron in the l<sup>th</sup> layer <sup>[nielsen reference]</sup>, essentially representing how the activation in each neuron contributes to the overall error. The error for a neuron can be written as follows:
                
                    
                    \[ \delta_j^l = \frac{\partial C}{\partial a_j^l} \cdot \sigma '(z_j^l) \]</p>
                    <p>For easier computation, we can rewrite the equation in a matrix-based form:
                        \[ \delta^L = (a^L - y) \odot \sigma'(z^L) \]
                        If we know the error for a layer, we can use the transpose of the weight matrix to calculate the error of the previous layer, as we effectively move the error backwards through the network:
                        \[ \delta^l = \left( (w^{l+1})^T \delta^{l+1} \right) \odot \sigma'(z^l) \]
                    </p>
                </div>
            </div>
            <div class="column">
                <div class="header"><h2 class="header">Applications</h2></div>
                <div class="text">
                    <p>
                        There are various types of machine learning, such as classification, regression, 
                        clustering, and dimensionality reduction. We have decided to develop a classification 
                        model that can be implemented in a banking environment to predict fraudulent transactions.
                    </p>
                    <div class="header"><h3 class="header">Justification</h3></div>
                    <p>Thinking about the outputs of the model, we would expect a yes/no answer from the model.
                    Therefore, a classification model is the most suitable to predict whether a transaction is fraudulent or not.
                    </p>
                    <div class="header"><h3 class="header">Selecting Model</h3></div>
                    <p>
                        There are many classification models that we could use, such as Random Forest, Support Vector Machine, Neural Networks, Decision Trees, kNN, etc.
                        We have decidede to use a supervised model. This means that the model learns from labeled data, meaning each input has a known output (e.g., fraud/not fraud). 
                        This is different to an unsupervised model which finds patterns in unlabeled data, such as grouping similar transactions without predefined fraud labels.
                        We have to choose a model that is suitable for our data and the problem we are trying to solve.
                        We have briefly look at many models, such as Support Vector Machine (SVM), linear regression, etc.
                        However we decided that kNN and Decision Tree are the most suitable for our data since they are easy to interpret, 
                        handle both numerical and categorical features well, and perform effectively with our dataset size and structure.

                    </p>
                </div>
                <div class="header"><h2 class="header">Classification</h2></div>
                <p>A classification model is a type of machine learning model that categorizes data into predefined classes, known as labels. It learns patterns from labeled training data and predicts the class of new, unseen data. For example, in fraud detection, the labels could be "fraudulent" or "genuine."
                    To build an effective model, the dataset is typically split into training, validation, and test sets to ensure the model generalizes well. A common split is training (70-80%), used for learning, and testing (20-30%), used for evaluation.
                    <br><br>Two common issues in classification are:</p>
                    <ul>
                        <li><b>Underfitting</b>: The model is too simple and fails to learn meaningful patterns, resulting in poor accuracy on both training and test data.</li>
                        <li><b>Overfitting</b>: The model learns the training data too well, capturing noise instead of general patterns, leading to poor performance on new data.</li>
                    </ul>
                    <div class="img">
                        <img
                            width="400"
                            src="fitting.png"
                            alt="Fitting" />
                        <p class="caption">
                            <sup ref author="Flask" date="31/03/2025" src="https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/"></sup>
                            Graph representation for overfitting and underfitting
                        </p>
                    </div>
                    <p>A well-balanced model avoids both overfitting and underfitting to ensure accurate predictions on real-world data.</p>
            </div>
            <div class="column">
                <div class="header"><h2 class="header">Analysing Data</h2></div>
                <p>To create an accurate model, we first needed to understand the problem: 
                    Which values in the dataset (we used synthetic data from Kaggle<sup ref author="Kaggle" date="31/03/2025" src="https://www.kaggle.com/datasets/kartik2112/fraud-detection/data"></sup>, 
                    which accurately represents real bank transactions) would be useful in predicting whether a transaction is fraudulent. 
                    We began our analysis by plotting various features (e.g., credit card numbers, transaction locations) against the fraud label to identify patterns that could help predict fraudulent transactions.</p>
                    <div class="img">
                        <img
                            width="400"
                            src="categories.png"
                            alt="percentage per categories" />
                    </div>
                    <p class="caption" style="text-align: center;">(Percentage of transactions per category graph)</p>
                    <div class="img">
                        <img
                            width="400"
                            height="140"
                            src="hour.png"
                            alt="hour against fraud" />
                    </div>
                    <p class="caption" style="text-align: center;">(Transactions time against time graph)</p>
                    <div class="img">
                        <img
                            width="400"
                            height="140"
                            src="amt.png"
                            alt="transaction amount against time" />
                    </div>
                    <p class="caption" style="text-align: center;">(Transactions amount against time graph)</p>
                    <p>
                        We have used matplotlib<sup ref author="matplotlib" date="31/03/2025" src="https://matplotlib.org"></sup> to plot the features against the fraud label and identify patterns that could help predict fraudulent transactions. 
                        We have found out that the time of transaction, the amount of transaction, and the category of transaction are the most important features to predict whether a transaction is fraudulent or not.
                        Therefore, we have used these features to train our model.
                    </p>
                <div class="header"><h2 class="header">KNN (k-Nearest Neighbours)</h2></div>
                <p>
                    The k-Nearest Neighbors (kNN) algorithm is a simple yet powerful machine learning model used for classification and regression. 
                    It is a non-parametric, instance-based learning method, meaning it does not explicitly learn a function during training. 
                    Instead, it memorizes the training data and makes predictions by comparing new data points to their nearest neighbors. 
                    The key idea is that similar data points tend to belong to the same category. 
                    The closeness between points is usually measured using Euclidean distance, given by the number of features. 
                    \[
                    d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
                    \]                    
                    The value of k determines how many neighbors are considered for classification; the most common class among these neighbors is assigned to the new data point.
                </p>
                <div class="img">
                    <img
                        width="300"
                        src="knn_code.png"
                        alt="knn_code" />
                </div>
                <p>
                    In our implementation, the dataset is first split into features (X) and labels (y). The training set (X_train, y_train) comes from balanced_df, 
                    ensuring that fraud and non-fraud cases are more evenly distributed to prevent bias. The test set (X_test, y_test) is used for evaluation. 
                    The kNN model is then initialized with k = 6 using KNeighborsClassifier(n_neighbors=6), 
                    meaning each transaction will be classified based on the majority class among its six nearest neighbors. 
                    Finally, we train the model using .fit(X_train, y_train), allowing it to store the training data for future predictions. 
                    To further improve performance, we can experiment with different values of n_neighbors.
                </p>
                <div class="header"><h2 class="header">Decision Tree</h2></div>
                <p>
                    Decision Trees are another non-parametric, supervised-learning based and easy-to-understand type of maching learning
                    model, used for classification and regression. This model learns certain rules that the data seems to follow to decide what the target
                    variable should be, based on the data given, and uses this to predict the label of new data, checking at different stages if certain
                    characteristics are true or false, as if passing the data through a flow chart.
                </p>
                <p>
                    Decision Trees are very efficient in terms of time, with a similar time complexity of to kNN. However, they're prone to overfitting: even the slightest change in the training data will result in
                    a completely different tree being generated. This wasn't an issue for our data, however, because ours wasn't being live-fed, so it wasn't changing at all.
                </p>
            </div>
            <div class="column">
                <div class="header"><h2 class="header">Evaluation</h2></div>
                <p>When measuring the efficiency of anything, the first method that comes to mind is measuring it's accuracy, the number of correct predictions over the total number of predictions. However, this isn't as useful as it seems. Say you have 100 training examples, 98 of which are not fraud and 2 of which are fraud.
                    A dummy model could predict all of these as not fraud and would end up with 98% accuracy, which seems high, but in fact the model is incapable of distinguishing between the 2 classes. As seen, this is mainly an issue when there is an imbalance in the amount of examples of each class.
                    <br><br> Other methods will measure, for example, how good the model is at predicting only fraud transactions, or how good it is at distinguishing between the two. These methods are: </p>
                    <div class="header"><h3 class="header">Precision & Recall</h3></div>
                    <p>Precision, also called True Positive Rate, is a measure of how good the model is at correctly identifying a positive case, in this case
                        identifying fraud alone. The formula for this is:
                        \[
                        Precision = \frac{TP}{TP+FP} 
                        ]\
                    </p>



                <div class="header"><h2 class="header">Frontend</h2></div>
                <p>
                    We have designed a transaction predictor webpage that allows users to upload a set of transaction data and we can determine if the transaction is fraudulent or genuine.
                    This was being done by using a Flask API<sup ref author="Flask" date="31/03/2025" src="https://flask.palletsprojects.com/en/stable/"></sup> to handle the backend, using joblib<sup ref author="joblib" date="31/03/2025" src="https://joblib.readthedocs.io/en/stable/"></sup> to load the pre-trained model and feature packages, finally using HTML, CSS and Javascript to connect to the backend.
                    It can be accessed at <a href="https://machinelearners.onrender.com">https://machinelearners.onrender.com</a>.
                </p>
                <div class="img">
                    <img
                        width="200"
                        src="frontend.png"
                        alt="Frontend" />
                </div>

                <p>
                    In the above example, a ridiculous set of data is being input to the transaction predictor:
                    Someone at 3 a.m. spending $696969 on food and dining, which gave us an output of a fraudulent transaction on both models.
                </p>
                <div class="header references"><h2>References</h2></div>
                <div id="references" class="text"><p></p></div>
            </div>
        </div>
    </body>
</html>
